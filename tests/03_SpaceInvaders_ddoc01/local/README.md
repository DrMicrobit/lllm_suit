# Local models that failed

## Too slow / memory hungry
- [olmo3-32B-think-unsloth](https://huggingface.co/unsloth/Olmo-3-32B-Think-GGUF): even 4 bit completely fills up my 32 GiB VRAM and leaves no space for context without spilling over to CPU. Maybe unoptimized Ollama atm?

## Fail with cline
